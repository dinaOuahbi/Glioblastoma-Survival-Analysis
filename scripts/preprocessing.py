# -*- coding: utf-8 -*-
"""Audrey_thesis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NxyNbEioRKyHLATNXCTk_K35CSPVFYRL
"""

'''
ANALYSE DE DONNEES 
- CANCER / GLIOME 
- AUTEUR DE SCRIPT / O DINA
- ETUDE / AUDREY (INTERNE EN NEUROCHIRURGIE)


LES DONNEES BRUTE ET TRAITEES SONT STOCKEES DANS K:\Bioinfo\Projets\Recherche\GLI\CLI\01\Donnees
LES RESULTAT SONT STOCKES DANS K:\Bioinfo\Projets\Recherche\GLI\CLI\01\Analyses_stats\results
'''

! pip install scikit-survival
! pip install lifelines

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
from sksurv.nonparametric import kaplan_meier_estimator
from sksurv.compare import compare_survival
import scipy.stats as stats
from scipy.stats import chi2
from rpy2.robjects.packages import importr
from rpy2.robjects.vectors import FloatVector

df_r = pd.read_excel('/content/GLI_rad.xlsx')
df_s = pd.read_excel('/content/GLI_surg.xlsx')

df_r['GROUP'] = 'rad'
df_s['GROUP'] = 'surg'
df = pd.concat([df_r, df_s])
df.head()

# INDEXATION PAR LES INITIALES DES PATIENTS
temp = list()
for i, j in zip(df['NOM'], df['PRENOM']):
  temp.append(f'{i[0]}{j[0]}')
df['INIT'] = temp
df.set_index('INIT', inplace=True)

# MODIFICATION LOC1 SELON AUDREY
df['loc1_bis'] = None
for patient in ['MG', 'DL', 'RJ', 'DM', 'BP']:
  df['loc1_bis'][patient] = 'NO_CHIR'

for patient in df.index:
  if patient not in ['MG', 'DL', 'RJ', 'DM', 'BP']:
    df['loc1_bis'][patient] = 'TYPICAL'

"""# Nouvelle section"""

# MODIFICATION LOC2 SELON AUDREY
df['loc2_bis'] = None
for patient in ['MG', 'LC', 'WJ', 'RP', 'DM', 'RJ', 'PL', 'BO', 'HJ', 'BP']:
  df['loc2_bis'][patient] = 'DEEP'

for patient in df.index:
  if patient not in ['MG', 'LC', 'WJ', 'RP', 'DM', 'RJ', 'PL', 'BO', 'HJ', 'BP']:
    df['loc2_bis'][patient] = 'NO_DEEP'

# MODIFICATION TYPECHIR1 SELON AUDREY
temp = list()
for i in df['TYPECHIR1']:
  if i == 'NON':
    temp.append(np.nan)
  else:
    temp.append(i)
df['TYPECHIR1_bis'] = temp

# MODIFICATION COTE SELON AUDREY
temp = list()
for i in df['COTE']:
  if i == 'DG':
    temp.append(np.nan)
  else:
    temp.append(i)
df['COTE'] = temp

# ENLEVER LES DEUX PATIENT DE LA RAD AYANT EU UNE CHIR2
df['TYPECHIR2']['DL'] = np.nan
df['TYPECHIR2']['BJ'] = np.nan

# INDEXING BY FIRST AND LAST NAME
df['ID'] = df[['NOM', 'PRENOM']].agg('_'.join, axis=1)
df.drop(['NOM', 'PRENOM'], axis=1, inplace=True)
df.set_index('ID', inplace=True)

# NA
plt.figure(figsize=(20,3))
plt.subplot(1,2,1)
df[df['GROUP'] =='surg'].isna().sum(axis=0).plot(kind='bar')
plt.grid(b='on')
plt.ylabel('NAN coutns (SURG)')
plt.subplots_adjust(wspace=0.6, hspace=0.6, left=0.1, bottom=0.22, right=0.96, top=0.96)

plt.subplot(1,2,2)
df[df['GROUP'] =='rad'].isna().sum(axis=0).plot(kind='bar')
plt.grid(b='on')
plt.ylabel('NAN coutns (RAD)')
plt.subplots_adjust(wspace=0.1, hspace=0.8, left=0.1, bottom=0.50, right=0.96, top=1)
plt.savefig('NAN_counts.jpg')

# KPS1 / KPS2
def replace_KPS(data_x, col, dich):
  temp = list()
  for i in data_x[col]:
    if i>dich:
      temp.append('HIGHT')
    elif i<=dich:
      temp.append('LOW')
    else:
      temp.append(i)
  data_x[col] = temp

replace_KPS(df, 'KPS1', 70)
replace_KPS(df, 'KPS2', 70)

# CTC2
for i in df['CTC2']:
  if i <= 40:
    df['CTC2'] = df['CTC2'].replace({i:'LOW'})
  elif i>=60 and i<=80:
    df['CTC2'] = df['CTC2'].replace({i:'MEDIUM'})
  else:
    df['CTC2'] = df['CTC2'].replace({i:'HIGHT'})

# convert to object 
for col in df.select_dtypes('float').columns.to_list():
  df[col]=df[col].astype('object')
  print(df[col].dtypes)

# LOC2
df['LOC2']=df['LOC2'].astype('object')

# DICHOTOMISER L'AGE
temp = list()
for i in df['AGE']:
  if i > df['AGE'].median():
    temp.append('OLDER')
  else:
    temp.append('YOUNGER')
df['DICH_AGE'] = temp

df.dtypes.value_counts().plot.pie()
plt.ylabel('')

# CREATE A SUPER VARIABLE CALLED COMORBIDITY ['DIABETE', 'CŒUR', 'REINS', 'IMMUNODEP', 'TABAC']
temp = list()
for row in range(df.shape[0]):
  my_list = df[['DIABETE', 'CŒUR', 'REINS', 'IMMUNODEP', 'TABAC']].iloc[row, :].tolist()
  temp.append(my_list.count('OUI'))
df['COMORBIDITY'] = temp

df['COMORBIDITY']=df['COMORBIDITY'].astype('object')

# select variables of intterest
new_df = df[['SEXE','AGE',
  'DICH_AGE',
  'COMORBIDITY',
  'EPILEPSIE',
  'AEG',
  'DEFICIT',
  'CTC2',
  'loc1_bis',
  'loc2_bis',
  'COTE',
  'IDH',
  'MGMT',
  'KPS1',
  'KPS2',
  'BIOPSIE1',
  'TYPECHIR1_bis',
  'TYPECHIR2',
  'COMPLICATION1',
  'COMPLICATION2',
  'DECES',
  'GROUP',
  'DATECHIR1',
  'DATERECIDIVE',
  'FINCHIMIO2',
  'DATEDECES']]

def generate_status(time_l):
  stat = list()
  for i in time_l:
    if i==0:
      stat.append(0)
    else:
      stat.append(1)
  return stat

def get_delta_time(df, start, end):
  d = df[end] - df[start]
  d = pd.Series(d).fillna(pd.Timedelta(days=0)).tolist()
  time_l = [i.days for i in d]
  status = generate_status(time_l)
  return time_l, status

def into_datetime(col):
  temp = list()
  for row in new_df[col]:
      if isinstance(row, str):
        new_date = pd.to_datetime(row.strip(), format='%d/%m/%Y') # ====> yyyy - mm - dd
        temp.append(new_date)
      else:
        temp.append(row)
  return temp

# colvert columns into datetime
new_df['DATECHIR1_bis'] = into_datetime('DATECHIR1')
new_df['DATERECIDIVE_bis'] = into_datetime('DATERECIDIVE')
new_df['FINCHIMIO2_bis'] = into_datetime('FINCHIMIO2')
new_df['DATEDECES_bis'] = into_datetime('DATEDECES')

# generqte times qnd status
new_df['PFS1'],new_df['PROG1'] = get_delta_time(new_df, 'DATECHIR1_bis', 'DATERECIDIVE_bis')
new_df['PFS2'], new_df['PROG2'] = get_delta_time(new_df, 'DATERECIDIVE_bis', 'FINCHIMIO2_bis')
new_df['OS1'], new_df['DCD1'] = get_delta_time(new_df, 'DATECHIR1_bis','DATEDECES_bis')
new_df['OS2'], new_df['DCD2'] = get_delta_time(new_df, 'DATERECIDIVE_bis','DATEDECES_bis')

# tester quelque dates
from datetime import datetime
def days_between(d1, d2):
    d1 = datetime.strptime(d1, "%Y-%m-%d")
    d2 = datetime.strptime(d2, "%Y-%m-%d")
    return abs((d2 - d1).days)

days_between('2020-05-18', '2020-09-28')

# REBOUILLAT_PASCAL prend la chimio2 avant sa premiere recidive ??? a revoir avec Audrey
print
(
    df['DATERECIDIVE'][df.index=='REBOUILLAT_PASCAL'],
    df['DATECHIMIO2'][df.index=='REBOUILLAT_PASCAL']
)

# finalement, changement de daterecidive ==> septembre

#df['OS2']['REBOUILLAT_PASCAL'] = abs(df['OS2']['REBOUILLAT_PASCAL'])

# remove space complication2
for i in new_df['COMPLICATION2']:
  new_df['COMPLICATION2'] = new_df['COMPLICATION2'].replace({i:i.strip()})
new_df['COMPLICATION2'].unique()

pd.set_option('display.max_columns', None)
new_df.head()

final_df = new_df.drop(['DATECHIR1','DATERECIDIVE','FINCHIMIO2','DATEDECES', 'DATECHIR1_bis','DATERECIDIVE_bis','FINCHIMIO2_bis','DATEDECES_bis'], axis=1)

final_df.dtypes.value_counts().plot.pie()
plt.ylabel('')
plt.savefig('dtypes.jpeg')

for col in final_df.select_dtypes('O'):
  print(col)
  print(final_df[col].unique())
  print('-'*50)

"""**DESCRIPTION**


*- les étapes d'analyse qui suivent ne sont pas a prendre en concideration car la meme chose à été faite sur RSTUDIO. neamoins, je ne voulais pas l'enlever pour garder une idée sur l'équivalent dans JUPYTER NOTEBOOK*
"""

final_df.to_csv('final_df.csv')

chi_test = pd.DataFrame(columns=['chi_square','p_value', 'dof'])
def dist(col, norm = False):
  all = final_df[col].value_counts(normalize=norm)
  surg = final_df[col][final_df['GROUP']=='surg'].value_counts(normalize=norm)
  rad = final_df[col][final_df['GROUP']=='rad'].value_counts(normalize=norm)
  print(all,'\n',
        surg,'\n',
        rad)
  
  table = np.array(pd.crosstab(final_df['GROUP'], final_df[col]))
  chi_test.loc[col, :] = stats.chi2_contingency(table)[:3]
  print(stats.chi2_contingency(table)[:3])

final_df.columns

for col in final_df.select_dtypes('O').columns:
  print(f'########{col}###################################################')
  print(final_df[col].dropna().shape[0])
  dist(col, norm = False)

for col in data_x.columns:
  print(f'---{col}----')
  print(data_x[col].dropna().value_counts())
  print(data_x[col][data_x['GROUP']=='surg'].dropna().value_counts())
  print(data_x[col][data_x['GROUP']=='rad'].dropna().value_counts())

def describe_variables(class_, df):
  describe = pd.DataFrame(index=['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])
  for col in df.select_dtypes('int'):
    describe[col] = df[col][df['GROUP']==class_].describe().values
  return describe

describe_variables('rad', df).to_csv('describe_rad_cohort.csv')

describe_variables('surg', df).to_csv('describe_surg_cohort.csv')

"""**chi2 test (categories)**"""

chi_test = pd.DataFrame(columns=['chi_square','p_value', 'dof'])
for col in data_x.columns:
  table = np.array(pd.crosstab(data_x['GROUP'], data_x[col]))
  chi_test.loc[col, :] = stats.chi2_contingency(table)[:3]

#adjust pvalue
stats = importr('stats')
chi_test['p_adjust'] = stats.p_adjust(FloatVector(chi_test['p_value'].to_list()), method = 'BH')
chi_test.to_csv('chi_square_test.csv')

def correct_pvalues_for_multiple_testing(pvalues, correction_type = "Benjamini-Hochberg"):                
    """                                                                                                   
    consistent with R - print correct_pvalues_for_multiple_testing([0.0, 0.01, 0.029, 0.03, 0.031, 0.05, 0.069, 0.07, 0.071, 0.09, 0.1]) 
    """
    from numpy import array, empty                                                                        
    pvalues = array(pvalues) 
    n = float(pvalues.shape[0])
    print(n)                                                                           
    new_pvalues = empty(int(n))
    if correction_type == "Bonferroni":                                                                   
        new_pvalues = n * pvalues
    elif correction_type == "Bonferroni-Holm":                                                            
        values = [ (pvalue, i) for i, pvalue in enumerate(pvalues) ]                                      
        values.sort()
        for rank, vals in enumerate(values):                                                              
            pvalue, i = vals
            new_pvalues[i] = (n-rank) * pvalue                                                            
    elif correction_type == "Benjamini-Hochberg":                                                         
        values = [ (pvalue, i) for i, pvalue in enumerate(pvalues) ]                                      
        values.sort()
        values.reverse()                                                                                  
        new_values = []
        for i, vals in enumerate(values):                                                                 
            rank = n - i
            pvalue, index = vals                                                                          
            new_values.append((n/rank) * pvalue)                                                          
        for i in range(0, int(n)-1):  
            if new_values[i] < new_values[i+1]:                                                           
                new_values[i+1] = new_values[i]                                                           
        for i, vals in enumerate(values):
            pvalue, index = vals
            new_pvalues[index] = new_values[i]                                                                                                                  
    return new_pvalues

correct_pvalues_for_multiple_testing(chi_test['p_value'].to_list(), correction_type = "Benjamini-Hochberg")

"""**wilconxon test (int)**"""

# show the violation between normality for some variables ===> because wilcoxon test ask for that
plt.figure(figsize=(10,10))
for i,col in enumerate(df.select_dtypes('int')):
  plt.subplot(3,2,i +1)
  stats.probplot(df[col], dist="norm", plot=plt)
  plt.title(f"{col} After Q-Q Plot")
  # set the spacing between subplots
plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4,  hspace=0.4)
plt.savefig("BP_After_QQ.png")

from scipy.stats import wilcoxon
import random
random.seed(1)
def W_test(final_df, col):
    data2 = np.array(final_df[col][final_df['GROUP'] == 'rad'])
    data1 = np.array(final_df[col][final_df['GROUP'] == 'surg'].sample(n = len(data2)))
    stat, p = wilcoxon(data1, data2)
    print('Statistics=%.3f, p=%.3f' % (stat, p))
    return p, data1, data2

p_val = dict()
for col in df.select_dtypes('int'):
    print(col)
    p, _, _ = W_test(df, col)
    p_val[col] = p

plt.figure(figsize=(15, 5))
for i, col in enumerate(df.select_dtypes('int')):
    p, data1, data2 = W_test(df, col)
    plt.subplot(2, 3, i +1)
    plt.boxplot([data1, data2], notch=True, patch_artist=True)
    plt.xticks([1, 2], ["Surgery", "Radio_therapy"])
    plt.title(f'{col} | p_val = {round(p, 5)}')
    plt.grid(True)
plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9,top=0.9,wspace=0.4,hspace=0.4)
plt.savefig('wc_boxplots.jpg')

"""survival **analysis**"""

data_y = df[['PROG1','PFS1']].to_records(index=False)
data_y

data_x.reset_index(drop=True,inplace=True)

def generate_km(PROG, PFS):
  data_y = df[[PROG,PFS]].to_records(index=False)
  time, survival_prob = kaplan_meier_estimator(data_y[PROG], data_y[PFS])
  plt.step(time, survival_prob, where="post", label=f'{PFS}')
  plt.ylabel("Probability of survival $\hat{S}(t)$")
  plt.xlabel("time $t$ (m)")
  plt.legend(loc="best")

plt.figure(figsize=(10,7))
generate_km('PROG1', 'PFS1')
generate_km('PROG2', 'PFS2')
generate_km('DCD1', 'OS1')
generate_km('DCD2', 'OS2')

def surv_diff(col, PFS, PROG, c1, c2):
  data_y = df[[PROG,PFS]].to_records(index=False)
  for value in (c1,c2):
    mask = data_x[col] == value
    time_treatment, survival_prob_treatment = kaplan_meier_estimator(
        data_y[PROG][mask],
        data_y[PFS][mask])
    plt.step(time_treatment, survival_prob_treatment, where="post",
             label=value)
  plt.ylabel("Probability of survival $\hat{S}(t)$")
  plt.xlabel("time $t$ (m)")
  plt.legend(loc="best")

from lifelines.statistics import logrank_test
def log_rank(col, PFS, PROG, c1, c2):
  data_y = df[[PROG,PFS]].to_records(index=False)
  T = data_y[PFS][data_x[col] == c1]
  T1 = data_y[PFS][data_x[col] == c2]
  E = data_y[PROG][data_x[col] == c1]
  E1 = data_y[PROG][data_x[col] == c2]
  res = logrank_test(T, T1, event_observed_A=E, event_observed_B=E1)
  return res

plt.figure(figsize=(15,10))
plt.subplot(2,2,1)
surv_diff('GROUP', 'PFS1', 'PROG1', 'surg', 'rad')
res = log_rank('GROUP', 'PFS1', 'PROG1', 'surg', 'rad')
plt.title(f'PFS1| p_val = {round(res.p_value, 5)}')

plt.subplot(2,2,2)
surv_diff('GROUP', 'PFS2', 'PROG2', 'surg', 'rad')
res = log_rank('GROUP', 'PFS2', 'PROG2', 'surg', 'rad')
plt.title(f'PFS2| p_val = {round(res.p_value, 5)}')

plt.subplot(2,2,3)
surv_diff('GROUP', 'OS1', 'DCD1', 'surg', 'rad')
res = log_rank('GROUP', 'OS1', 'DCD1', 'surg', 'rad')
plt.title(f'OS1| p_val = {round(res.p_value, 5)}')

plt.subplot(2,2,4)
surv_diff('GROUP', 'OS2', 'DCD2', 'surg', 'rad')
res = log_rank('GROUP', 'OS2', 'DCD2', 'surg', 'rad')
plt.title(f'OS2| p_val = {round(res.p_value, 5)}')

"""**univariate analysis (PFS1)**"""

def convert_types(df,cat_names):
      dtypes = {cat:'category' for cat in cat_names}
      df = df.astype(dtypes)
      return df

from sksurv.preprocessing import OneHotEncoder
data_x_numeric = OneHotEncoder().fit_transform(convert_types(data_x, data_x.columns))
data_x_numeric.head()

from sksurv.linear_model import CoxPHSurvivalAnalysis
test_x = data_x_numeric[data_x_numeric.columns[data_x_numeric.isna().sum() == 0]]
estimator = CoxPHSurvivalAnalysis()
estimator.fit(test_x, data_y)

pd.Series(estimator.coef_, index=test_x.columns)

surv_diff('GROUP', 'PFS1', 'PROG1', 'surg', 'rad')
res = log_rank('GROUP', 'PFS1', 'PROG1', 'surg', 'rad')
plt.title(f'PFS1| p_val = {round(res.p_value, 5)}')